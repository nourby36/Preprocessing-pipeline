{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rZo2xfhidkqu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install transformers\n",
        "!pip install pycountry\n",
        "!pip install tldextract\n",
        "!pip install python-whois\n",
        "!pip install ipinfo\n",
        "!pip install requests\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install glob2\n",
        "!pip install nltk\n",
        "!pip install pytz\n",
        "!pip install dateparser\n",
        "!pip install langdetect\n",
        "!pip install lingua-language-detector\n",
        "!pip install requests beautifulsoup4 tldextract pycountry\n",
        "!pip install python-whois\n",
        "\n",
        "# Downloading spaCy language models\n",
        "!python -m spacy download xx_ent_wiki_sm\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Downloading NLTK stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "8HolJln8yYZJ",
        "outputId": "adb6489a-51af-4ac4-c809-707da623538c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting pycountry\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry\n",
            "Successfully installed pycountry-24.6.1\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.7)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.15.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2024.7.4)\n",
            "Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-2.1.0 tldextract-5.1.2\n",
            "Collecting python-whois\n",
            "  Downloading python_whois-0.9.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from python-whois) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->python-whois) (1.16.0)\n",
            "Downloading python_whois-0.9.4-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-whois\n",
            "Successfully installed python-whois-0.9.4\n",
            "Collecting ipinfo\n",
            "  Downloading ipinfo-5.0.1-py3-none-any.whl.metadata (648 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ipinfo) (2.31.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from ipinfo) (5.4.0)\n",
            "Requirement already satisfied: aiohttp<=4 in /usr/local/lib/python3.10/dist-packages (from ipinfo) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=4->ipinfo) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=4->ipinfo) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=4->ipinfo) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=4->ipinfo) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=4->ipinfo) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<=4->ipinfo) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ipinfo) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ipinfo) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ipinfo) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ipinfo) (2024.7.4)\n",
            "Downloading ipinfo-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: ipinfo\n",
            "Successfully installed ipinfo-5.0.1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.1.4)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.10/dist-packages (0.7)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (2024.1)\n",
            "Collecting dateparser\n",
            "  Downloading dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser) (2024.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser) (2024.5.15)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser) (5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser) (1.16.0)\n",
            "Downloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dateparser\n",
            "Successfully installed dateparser-1.2.0\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=9497e530547413aa723677313fb210a911897ff07c262587e5e9de75cf0e6757\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting lingua-language-detector\n",
            "  Downloading lingua_language_detector-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (349 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.2/349.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lingua_language_detector-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (74.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lingua-language-detector\n",
            "Successfully installed lingua-language-detector-2.0.2\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (5.1.2)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.10/dist-packages (24.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.15.4)\n",
            "Requirement already satisfied: python-whois in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from python-whois) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->python-whois) (1.16.0)\n",
            "Collecting xx-ent-wiki-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.7.0/xx_ent_wiki_sm-3.7.0-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from xx-ent-wiki-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: xx-ent-wiki-sm\n",
            "Successfully installed xx-ent-wiki-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_ent_wiki_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 tldextract pycountry python-whois geoip2\n",
        "!pip install stopwords\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrNwMtveOvB8",
        "outputId": "8b42739e-bd88-49a9-89ed-21e0608fc7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (5.1.2)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.10/dist-packages (24.6.1)\n",
            "Requirement already satisfied: python-whois in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Collecting geoip2\n",
            "  Downloading geoip2-4.8.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.15.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from python-whois) (2.8.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from geoip2) (3.9.5)\n",
            "Collecting maxminddb<3.0.0,>=2.5.1 (from geoip2)\n",
            "  Downloading maxminddb-2.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: setuptools>=60.0.0 in /usr/local/lib/python3.10/dist-packages (from geoip2) (71.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.6.2->geoip2) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->python-whois) (1.16.0)\n",
            "Downloading geoip2-4.8.0-py2.py3-none-any.whl (27 kB)\n",
            "Downloading maxminddb-2.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: maxminddb, geoip2\n",
            "Successfully installed geoip2-4.8.0 maxminddb-2.6.2\n",
            "Collecting stopwords\n",
            "  Downloading stopwords-1.0.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Downloading stopwords-1.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Installing collected packages: stopwords\n",
            "Successfully installed stopwords-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import datetime\n",
        "from dateutil import parser\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "import glob\n",
        "import tldextract\n",
        "import pycountry\n",
        "from urllib.parse import urlparse\n",
        "import socket\n",
        "import whois\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from lingua import LanguageDetectorBuilder\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "\n",
        "from typing import List, Dict, Any,Tuple\n",
        "\n",
        "import logging\n",
        "\n",
        "import nltk\n",
        "from stopwords import get_stopwords\n",
        "\n"
      ],
      "metadata": {
        "id": "RYao-ty27OYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spacy model for multi-language support\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "multi_lang_nlp = spacy.blank(\"xx\")"
      ],
      "metadata": {
        "id": "jZWNwO4aM2tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Remove duplicates function***"
      ],
      "metadata": {
        "id": "a3-8SNlBdwYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to hash a row\n",
        "def hash_row(row: Dict[str, Any]) -> str:\n",
        "    return hashlib.md5(json.dumps(row, sort_keys=True).encode()).hexdigest()\n",
        "\n",
        "# Function to process a list of JSON dictionaries and remove duplicates\n",
        "def process_json_files(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        unique_data = []\n",
        "        existing_hashes = set()\n",
        "\n",
        "        for item in data:\n",
        "            item_hash = hash_row(item)\n",
        "            if item_hash not in existing_hashes:\n",
        "                unique_data.append(item)\n",
        "                existing_hashes.add(item_hash)\n",
        "\n",
        "        return unique_data\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing data: {e}\")\n",
        "        return data"
      ],
      "metadata": {
        "id": "KMHIxGqodvZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***keywords extraction function***"
      ],
      "metadata": {
        "id": "GU_OcmHQrd8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ISO language codes to NLTK stopwords language names\n",
        "LANGUAGE_CODE_MAP = {\n",
        "    'en': 'english',\n",
        "    'de': 'german',\n",
        "    'fr': 'french',\n",
        "    'es': 'spanish',\n",
        "    'it': 'italian',\n",
        "    'nl': 'dutch',\n",
        "    'pt': 'portuguese',\n",
        "    'ru': 'russian',\n",
        "    'sv': 'swedish',\n",
        "    'no': 'norwegian',\n",
        "    'da': 'danish',\n",
        "    'fi': 'finnish',\n",
        "    'pl': 'polish',\n",
        "    'cs': 'czech',\n",
        "    'sk': 'slovak',\n",
        "    'hu': 'hungarian',\n",
        "    'ro': 'romanian',\n",
        "    'el': 'greek',\n",
        "}\n",
        "\n",
        "# Function to detect language\n",
        "def detect_language(query: str) -> str:\n",
        "    try:\n",
        "        return detect(query)\n",
        "    except Exception as e:\n",
        "        print(f\"Error detecting language: {e}\")\n",
        "        return 'en'  # Default to English if detection fails\n",
        "\n",
        "# Function to get stop words for a given language\n",
        "def get_stop_words(language_code: str) -> set:\n",
        "    try:\n",
        "        language = LANGUAGE_CODE_MAP.get(language_code, None)\n",
        "        if language:\n",
        "            return set(nltk_stopwords.words(language))\n",
        "        else:\n",
        "            return set(get_stopwords(language_code))\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting stop words: {e}\")\n",
        "        return set(nltk_stopwords.words('english'))  # Default to English stop words if language is not supported\n",
        "\n",
        "# Function to preprocess text by removing stop words\n",
        "def preprocess_text(text: str, language_code: str) ->  Tuple[str, str]:\n",
        "    try:\n",
        "        stop_words = get_stop_words(language_code)\n",
        "        tokens = [word for word in text.lower().split() if word not in stop_words]\n",
        "        return ' '.join(tokens), stop_words\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing text: {e}\")\n",
        "        return text, set()\n",
        "\n",
        "# Function to extract keywords\n",
        "def extract_keywords(text: str, num_keywords: int = 10) -> List[str]:\n",
        "    try:\n",
        "        tokens = text.split()\n",
        "        word_freq = Counter(tokens)\n",
        "        keywords = word_freq.most_common(num_keywords)\n",
        "        return [keyword[0] for keyword in keywords]\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting keywords: {e}\")\n",
        "        return []\n",
        "\n",
        "# Extract keywords for each product's content and update the JSON data\n",
        "def keywords_extraction(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        for product in data:\n",
        "            content = product.get(\"content\", \"\")\n",
        "            language = detect_language(content)\n",
        "            preprocessed_content, stop_words = preprocess_text(content, language)\n",
        "            keywords = extract_keywords(preprocessed_content)\n",
        "            product['keywords'] = keywords  # Add keywords as a new field\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error in keywords extraction: {e}\")\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "fXgku9bjrVC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Inactive products detection fucntion***\n"
      ],
      "metadata": {
        "id": "PUG_HHq_eGAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_inactive_products(data: List[Dict[str, Any]], months_threshold: int) -> List[Dict[str, Any]]:\n",
        "  \"\"\"\n",
        "  Detect inactive products based on the last updated date.\n",
        "\n",
        "  Parameters:\n",
        "  data (List[Dict[str, Any]]): List of dictionaries where each dictionary represents a product with various attributes.\n",
        "  months_threshold (int): The threshold in months to determine if a product is inactive.\n",
        "\n",
        "  Returns:\n",
        "  List[Dict[str, Any]]: The input list with an additional key 'inactive_product' in each product dictionary indicating\n",
        "                        whether the product is inactive (True) or not (False).\n",
        "\n",
        "  Each product dictionary is expected to have the following structure:\n",
        "  {\n",
        "      'id': Any,              # Unique identifier for the product\n",
        "      'updated_at': str,      # ISO 8601 date string indicating the last update time of the product\n",
        "      ...                     # Other product attributes\n",
        "  }\n",
        "\n",
        "  The function adds a new key 'inactive_product' to each dictionary:\n",
        "  - 'inactive_product': bool - True if the product has not been updated within the specified threshold, False otherwise.\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the threshold date by subtracting the specified number of months from the current date\n",
        "  threshold_date = datetime.datetime.now() - datetime.timedelta(days=months_threshold * 30)\n",
        "\n",
        "  for product in data:\n",
        "    # Get the 'updated_at' value from the product dictionary\n",
        "    updated_at = product.get('updated_at')\n",
        "\n",
        "    if updated_at:\n",
        "        try:\n",
        "          # Parse the 'updated_at' date string to a datetime object\n",
        "          updated_at = datetime.datetime.strptime(updated_at, '%Y-%m-%dT%H:%M:%S%z')\n",
        "          updated_at = updated_at.replace(tzinfo=None)  # Remove timezone info\n",
        "\n",
        "          # Determine if the product is inactive based on the threshold date\n",
        "          product['inactive_product'] = updated_at < threshold_date\n",
        "        except ValueError:\n",
        "          # Handle invalid date format by printing an error message and marking the product as active\n",
        "          print(f\"Invalid date format for product: {product.get('id')}\")\n",
        "          product['inactive_product'] = False\n",
        "    else:\n",
        "      # Mark product as inactive if 'updated_at' is missing\n",
        "      product['inactive_product'] = True\n",
        "\n",
        "  # Return the updated list of products\n",
        "  return data"
      ],
      "metadata": {
        "id": "tycsLaP3eEYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Price standardization***"
      ],
      "metadata": {
        "id": "H3b4L0y0saei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def get_conversion_rates():\n",
        "    API_KEY = 'fb40b49fb6b179ac81766c0f'\n",
        "    url = f'https://v6.exchangerate-api.com/v6/{API_KEY}/latest/USD'\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if data['result'] == 'success':\n",
        "            return data['conversion_rates']\n",
        "        else:\n",
        "            logging.error(\"Failed to fetch conversion rates.\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Request error: {e}\")\n",
        "        return None\n",
        "\n",
        "def convert_to_usd(price_str, conversion_rates):\n",
        "    try:\n",
        "        # Dictionary for currency symbols\n",
        "        currency_symbols = {\n",
        "            '€': 'EUR',\n",
        "            '£': 'GBP',\n",
        "            '¥': 'JPY',\n",
        "            '$': 'USD',\n",
        "            'C$': 'CAD',\n",
        "            'A$': 'AUD',\n",
        "            'Fr.': 'CHF',\n",
        "            '¥': 'CNY',\n",
        "            'د.ت': 'TND',\n",
        "            'د.إ': 'AED',\n",
        "            'ر.س': 'SAR',\n",
        "            'د.ك': 'KWD',\n",
        "            'ر.ق': 'QAR',\n",
        "            'ر.ع': 'OMR'\n",
        "            # Add more symbols as needed\n",
        "        }\n",
        "\n",
        "        # Regular expression to match price and currency\n",
        "        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*([\\D]+)?', price_str)\n",
        "        if not match:\n",
        "            logging.error(f\"Invalid price format: {price_str}\")\n",
        "            return None, None, None\n",
        "\n",
        "        amount = float(match.group(1))\n",
        "        currency_identifier = match.group(2).strip() if match.group(2) else 'USD'\n",
        "\n",
        "        # Convert symbol to currency code if necessary\n",
        "        currency = currency_symbols.get(currency_identifier, currency_identifier)\n",
        "\n",
        "        if currency in conversion_rates:\n",
        "            if currency != 'USD':\n",
        "                usd_price = amount / conversion_rates[currency]\n",
        "                return round(usd_price, 2), 'USD', currency\n",
        "            else:\n",
        "                return amount, 'USD', currency\n",
        "        else:\n",
        "            logging.warning(f\"Unknown currency: {currency}. Assuming USD.\")\n",
        "            return amount, 'USD', currency\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting to USD: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def process_products_for_usd(products: List[Dict[str, Any]], conversion_rates: Dict[str, float]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        processed_products = []\n",
        "        products_changed = 0\n",
        "        products_in_usd = 0\n",
        "\n",
        "        for product in products:\n",
        "            new_product = product.copy()\n",
        "            product_changed = False\n",
        "            for variant in new_product.get('variants', []):\n",
        "                original_price = variant['price']\n",
        "                new_price, new_currency, original_currency = convert_to_usd(original_price, conversion_rates)\n",
        "                if new_price is not None:\n",
        "                    if original_currency != 'USD':\n",
        "                        if not product_changed:\n",
        "                            product_name = new_product.get('name')\n",
        "                            if product_name:\n",
        "                                print(f\"  Product: {product_name}\")\n",
        "                            else:\n",
        "                                print(f\"  Product: (Name not found - ID: {new_product.get('id', 'ID not found')})\")\n",
        "                            product_changed = True\n",
        "                        print(f\"    Changed {original_price} to {new_price:.2f} USD\")\n",
        "                        variant['price_in_USD'] = f\"{new_price:.2f}\"\n",
        "                        products_changed += 1\n",
        "                    else:\n",
        "                        products_in_usd += 1\n",
        "                        variant['price_in_USD'] = f\"{original_price}\"\n",
        "                else:\n",
        "                    logging.error(f\"Couldn't convert price: {original_price}\")\n",
        "\n",
        "            new_product['currency'] = 'USD'\n",
        "            processed_products.append(new_product)\n",
        "\n",
        "        return processed_products\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing products for USD: {e}\")\n",
        "        return products\n",
        "\n",
        "def standardize_prices_to_usd(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        conversion_rates = get_conversion_rates()\n",
        "        if not conversion_rates:\n",
        "            logging.error(\"Conversion rates not available. Exiting.\")\n",
        "            return products\n",
        "\n",
        "        processed_products = process_products_for_usd(products, conversion_rates)\n",
        "        return processed_products\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in standardizing prices to USD: {e}\")\n",
        "        return products\n"
      ],
      "metadata": {
        "id": "N30BQfEYslIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Country and region from URL extraction function***\n",
        "get_ip_geolocation ---> Requests per month 50k ?"
      ],
      "metadata": {
        "id": "pFT8TZN5tj3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tldextract\n",
        "import pycountry\n",
        "import whois\n",
        "from urllib.parse import urlparse\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def extract_seller_information(data: List[Dict[str, Any]]) -> List[str]:\n",
        "    sellers = []\n",
        "    for product in data:\n",
        "        if 'product_url' in product:\n",
        "            sellers.append(product['product_url'])\n",
        "    return sellers\n",
        "\n",
        "def get_country_from_tld(url: str) -> Tuple[str, str]:\n",
        "    ext = tldextract.extract(url)\n",
        "    tld = ext.suffix\n",
        "    try:\n",
        "        country = pycountry.countries.lookup(tld).name\n",
        "    except LookupError:\n",
        "        country = None\n",
        "    return country, None\n",
        "\n",
        "def get_country_and_region_from_whois(url: str) -> Tuple[str, str]:\n",
        "    try:\n",
        "        domain = urlparse(url).netloc\n",
        "        domain_info = whois.whois(domain)\n",
        "        country = domain_info.get('country')\n",
        "        region = domain_info.get('state') or domain_info.get('province')\n",
        "        return country, region\n",
        "    except Exception as e:\n",
        "        print(f\"WHOIS lookup failed for {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def get_country_and_region(url: str) -> Tuple[str, str]:\n",
        "    country, region = get_country_from_tld(url)\n",
        "    if country:\n",
        "        return country, region\n",
        "\n",
        "    country, region = get_country_and_region_from_whois(url)\n",
        "    if country:\n",
        "        return country, region\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def update_json_with_location(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    updated_data = []\n",
        "    try:\n",
        "        for product in data:\n",
        "            seller_url = product.get('product_url', '')\n",
        "            country, region = get_country_and_region(seller_url)\n",
        "            product['country'] = country if country else 'Not found'\n",
        "            product['region'] = region if region else 'Not found'\n",
        "            updated_data.append(product)\n",
        "            # Print the results for each URL\n",
        "            print(f\"URL: {seller_url}\")\n",
        "            print(f\"Country: {product['country']}\")\n",
        "            print(f\"Region: {product['region']}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while updating the JSON data: {e}\")\n",
        "        return data  # Return the original data with missing fields\n",
        "\n",
        "    return updated_data"
      ],
      "metadata": {
        "id": "jlP4mj3Att1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Discount extraction function***"
      ],
      "metadata": {
        "id": "gIzazgSdLk7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_discounts_and_add_info(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Extract discount information from product data and add additional information.\n",
        "\n",
        "    Parameters:\n",
        "    data (List[Dict[str, Any]]): List of dictionaries where each dictionary represents a product with various attributes.\n",
        "\n",
        "    Returns:\n",
        "    List[Dict[str, Any]]: The input list with additional keys 'discount_code' and 'discounts' in each product dictionary.\n",
        "\n",
        "    Each product dictionary is expected to have the following structure:\n",
        "    {\n",
        "        'id': Any,              # Unique identifier for the product\n",
        "        'title': str,           # Title of the product\n",
        "        'variants': List[Dict[str, Any]]  # List of variant dictionaries, each containing 'price' and optionally 'compare_at_price'\n",
        "        ...                     # Other product attributes\n",
        "    }\n",
        "\n",
        "    The function adds new keys to each product dictionary:\n",
        "    - 'discount_code': str - A fixed discount code 'DISCOUNT2024'.\n",
        "    - 'discounts': List[Dict[str, Any]] - A list of discount details for each variant with a 'compare_at_price' higher than 'price'.\n",
        "        Each discount dictionary contains:\n",
        "        {\n",
        "            'price': float,                   # Current price of the variant\n",
        "            'compare_at_price': float,        # Original price of the variant before discount\n",
        "            'discount': float,                # Discount amount\n",
        "            'discount_percentage': float      # Discount percentage\n",
        "        }\n",
        "    \"\"\"\n",
        "    try:\n",
        "        discounts = []\n",
        "        for product in data:\n",
        "            # Add discount code to the product\n",
        "            product['discount_code'] = 'DISCOUNT2024'\n",
        "\n",
        "            # Initialize the discounts list for the product\n",
        "            product['discounts'] = []\n",
        "\n",
        "            # Iterate over each variant in the product\n",
        "            for variant in product.get('variants', []):\n",
        "                price = float(variant['price'])\n",
        "                compare_at_price = variant.get('compare_at_price')\n",
        "\n",
        "                # Check if compare_at_price is present and not zero\n",
        "                if compare_at_price and float(compare_at_price) != 0:\n",
        "                    compare_at_price = float(compare_at_price)\n",
        "                    discount = compare_at_price - price\n",
        "                    discount_percentage = (discount / compare_at_price) * 100\n",
        "\n",
        "                    # Add discount_percentage to the variant dictionary\n",
        "                    variant['discount_percentage'] = discount_percentage\n",
        "\n",
        "                    # Append discount details to the product's discounts list\n",
        "                    product['discounts'].append({\n",
        "                        'price': price,\n",
        "                        'compare_at_price': compare_at_price,\n",
        "                        'discount': discount,\n",
        "                        'discount_percentage': discount_percentage\n",
        "                    })\n",
        "\n",
        "                    # Append discount details to the overall discounts list\n",
        "                    discounts.append({\n",
        "                        'product_id': product['id'],\n",
        "                        'title': product['title'],\n",
        "                        'price': price,\n",
        "                        'compare_at_price': compare_at_price,\n",
        "                        'discount': discount,\n",
        "                        'discount_percentage': discount_percentage\n",
        "                    })\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions that occur during processing\n",
        "        for product in data:\n",
        "            if 'discount_code' not in product:\n",
        "                product['discount_code'] = 'DISCOUNT2024'\n",
        "            if 'discounts' not in product:\n",
        "                product['discounts'] = []\n",
        "\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return data"
      ],
      "metadata": {
        "id": "Ad_UkH8GLICE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Min max prices***"
      ],
      "metadata": {
        "id": "2JtZGYL3t_AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_min_max_prices(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        for product in data:\n",
        "            prices = [float(variant[\"price_in_USD\"]) for variant in product.get(\"variants\", [])]\n",
        "            if prices:\n",
        "                product[\"min_price\"] = min(prices)\n",
        "                product[\"max_price\"] = max(prices)\n",
        "            else:\n",
        "                product[\"min_price\"] = product[\"max_price\"] = None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing data: {e}\")\n",
        "        raise\n",
        "    return data"
      ],
      "metadata": {
        "id": "LXtNGoEWt978"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Clean options function***"
      ],
      "metadata": {
        "id": "n474ot_nuGsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_default_title(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        for product in products:\n",
        "            if \"options\" in product:\n",
        "                for option in product[\"options\"]:\n",
        "                    option[\"values\"] = [None if value and value.lower() == \"default title\" else value for value in option[\"values\"]]\n",
        "            if \"variants\" in product:\n",
        "                for variant in product[\"variants\"]:\n",
        "                    if variant[\"title\"].strip() and variant[\"title\"].lower().strip() == \"default title\":\n",
        "                        variant[\"title\"] = product[\"title\"]\n",
        "                    if variant[\"option1\"].strip() and variant[\"option1\"].lower().strip() == \"default title\":\n",
        "                        variant[\"option1\"] = None\n",
        "                    if variant[\"option2\"].strip() and variant[\"option2\"].lower().strip() == \"default title\":\n",
        "                        variant[\"option2\"] = None\n",
        "                    if variant[\"option3\"].strip() and variant[\"option3\"].lower().strip() == \"default title\":\n",
        "                        variant[\"option3\"] = None\n",
        "        return products\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while replacing default titles: {e}\")\n",
        "        return products  # Returning the original products list in case of an error\n"
      ],
      "metadata": {
        "id": "GEfMzYAeuE47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Freshness score function***\n"
      ],
      "metadata": {
        "id": "fZ5knk1UecBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paramètres configurables\n",
        "MAX_DAYS = 365\n",
        "WEIGHTS = {'created': 0.2, 'published': 0.3, 'updated': 0.5}\n",
        "\n",
        "def parse_date(date_str: str) -> datetime.datetime:\n",
        "    try:\n",
        "        dt = parser.parse(date_str)\n",
        "        # Ensure all datetime objects are timezone-naive (convert to UTC)\n",
        "        if dt.tzinfo is not None:\n",
        "            dt = dt.astimezone(datetime.timezone.utc).replace(tzinfo=None)\n",
        "        return dt\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors du parsing de la date {date_str}: {e}\")\n",
        "        return None\n",
        "\n",
        "def freshness_score(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    for product in data:\n",
        "        try:\n",
        "            created_date = parse_date(product.get('created_at'))\n",
        "            published_date = parse_date(product.get('published_at'))\n",
        "            updated_date = parse_date(product.get('updated_at'))\n",
        "\n",
        "            scores = []\n",
        "            now = datetime.datetime.now()\n",
        "\n",
        "            if created_date:\n",
        "                days_since_created = (now - created_date).days\n",
        "                scores.append(WEIGHTS['created'] * (MAX_DAYS - min(days_since_created, MAX_DAYS)) / MAX_DAYS)\n",
        "            if published_date:\n",
        "                days_since_published = (now - published_date).days\n",
        "                scores.append(WEIGHTS['published'] * (MAX_DAYS - min(days_since_published, MAX_DAYS)) / MAX_DAYS)\n",
        "            if updated_date:\n",
        "                days_since_updated = (now - updated_date).days\n",
        "                scores.append(WEIGHTS['updated'] * (MAX_DAYS - min(days_since_updated, MAX_DAYS)) / MAX_DAYS)\n",
        "\n",
        "            product['freshness_score'] = sum(scores) if scores else None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors du calcul du score de fraîcheur pour le produit {product.get('id', 'ID inconnu')}: {e}\")\n",
        "            product['freshness_score'] = None\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "06rIqDhJeaGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Base id generation***"
      ],
      "metadata": {
        "id": "cYi8CVUQEG0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_products(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        # Generate baseid for each product\n",
        "        for product in data:\n",
        "            product_title = product['title']\n",
        "            vendor = product.get('vendor', 'unknown_vendor')\n",
        "            product_id = product.get('id', 'no_id')\n",
        "\n",
        "            # Generate initials and hash for baseid\n",
        "            title_initials = ''.join([word[0].upper() for word in product_title.split() if word])\n",
        "            vendor_initials = ''.join([word[0].upper() for word in vendor.split() if word])\n",
        "            id_hash = hashlib.md5(str(product_id).encode()).hexdigest()[:6]\n",
        "\n",
        "            baseid = f\"{title_initials}{vendor_initials}-{id_hash}\"\n",
        "            product['baseid'] = baseid\n",
        "\n",
        "            for variant in product.get('variants', []):\n",
        "                variant['baseid'] = baseid\n",
        "\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        # Add missing fields to original data if any exception occurs\n",
        "        for product in data:\n",
        "            if 'baseid' not in product:\n",
        "                product['baseid'] = None\n",
        "            for variant in product.get('variants', []):\n",
        "                if 'baseid' not in variant:\n",
        "                    variant['baseid'] = None\n",
        "        return data"
      ],
      "metadata": {
        "id": "WLi8lAK6EEyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Seller information from products extraction function*** (Unscalable)"
      ],
      "metadata": {
        "id": "rZo2xfhidkqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract seller URLs from the JSON data\n",
        "def extract_seller_information(file_path):\n",
        "    # Load the JSON file\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Extracting the seller URLs from each product\n",
        "    sellers = {product['product_url'] for product in data}\n",
        "\n",
        "    return data, sellers\n",
        "\n",
        "# Function to scrape seller information from URLs\n",
        "def scrape_seller_info(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve the webpage: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Example scraping logic (adjust based on actual site structure)\n",
        "    seller_info = {}\n",
        "\n",
        "    # Assuming there's a contact page link in the footer or header\n",
        "    contact_link = soup.find('a', href=True, text=re.compile('Contact', re.IGNORECASE))\n",
        "    if contact_link:\n",
        "        contact_page_url = contact_link['href']\n",
        "        if not contact_page_url.startswith('http'):\n",
        "            contact_page_url = url + contact_page_url\n",
        "        contact_response = requests.get(contact_page_url)\n",
        "        contact_soup = BeautifulSoup(contact_response.content, 'html.parser')\n",
        "\n",
        "        # Extract contact details from the contact page\n",
        "        email = contact_soup.find('a', href=re.compile('mailto:', re.IGNORECASE))\n",
        "        phone = contact_soup.find('a', href=re.compile('tel:', re.IGNORECASE))\n",
        "        address = contact_soup.find('p', text=re.compile('address', re.IGNORECASE))\n",
        "\n",
        "        seller_info['email'] = email['href'].replace('mailto:', '') if email else 'Not found'\n",
        "        seller_info['phone'] = phone['href'].replace('tel:', '') if phone else 'Not found'\n",
        "        seller_info['address'] = address.get_text(strip=True) if address else 'Not found'\n",
        "\n",
        "    # Extract social media links\n",
        "    social_media = soup.find_all('a', href=True)\n",
        "    social_links = {link.text.strip(): link['href'] for link in social_media if 'facebook' in link['href'] or 'instagram' in link['href']}\n",
        "    seller_info['social_media'] = social_links\n",
        "\n",
        "    return seller_info\n",
        "\n",
        "# Main function to add seller info to JSON data and save it\n",
        "def add_seller_info_to_json(file_path):\n",
        "    data, sellers = extract_seller_information(file_path)\n",
        "\n",
        "    for product in data:\n",
        "        seller_url = product['product_url']\n",
        "        seller_info = scrape_seller_info(seller_url)\n",
        "        product['seller_info'] = seller_info\n",
        "\n",
        "    # Save the updated JSON data back to the file\n",
        "    with open(file_path, 'w') as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q7xaZqWkdffF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Final function***"
      ],
      "metadata": {
        "id": "EHuVWNkyuPyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir data"
      ],
      "metadata": {
        "id": "tP8Yq0TcqBmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Edit"
      ],
      "metadata": {
        "id": "UcAycjmiud8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "\n",
        "#country and region detection\n",
        "def update_json_with_location(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    sellers = extract_seller_information(data)\n",
        "\n",
        "    for product, seller_url in zip(data, sellers):\n",
        "        try:\n",
        "            country, region = get_country_and_region(seller_url)\n",
        "            product['country'] = country if country else 'Not found'\n",
        "            product['region'] = region if region else 'Not found'\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {seller_url}: {e}\")\n",
        "            product['country'] = 'Not found'\n",
        "            product['region'] = 'Not found'\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "#keywords extraction\n",
        "def keywords_extraction(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        for product in data:\n",
        "            content = product.get(\"content\", \"\")\n",
        "            language = detect_language(content)\n",
        "            preprocessed_content, stop_words = preprocess_text(content, language)\n",
        "            keywords = extract_keywords(preprocessed_content)\n",
        "            product['keywords'] = keywords  # Add keywords as a new field\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error in keywords extraction: {e}\")\n",
        "        return data\n",
        "\n",
        "#duplicate removal\n",
        "def process_json_files(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        unique_data = []\n",
        "        existing_hashes = set()\n",
        "\n",
        "        for item in data:\n",
        "            item_hash = hash_row(item)\n",
        "            if item_hash not in existing_hashes:\n",
        "                unique_data.append(item)\n",
        "                existing_hashes.add(item_hash)\n",
        "\n",
        "        return unique_data\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing data: {e}\")\n",
        "        return data\n",
        "\n",
        "#min max price\n",
        "def add_min_max_prices(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        for product in data:\n",
        "            prices = [float(variant[\"price_in_USD\"]) for variant in product.get(\"variants\", [])]\n",
        "            if prices:\n",
        "                product[\"min_price\"] = min(prices)\n",
        "                product[\"max_price\"] = max(prices)\n",
        "            else:\n",
        "                product[\"min_price\"] = product[\"max_price\"] = None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing data: {e}\")\n",
        "        raise\n",
        "    return data\n",
        "\n",
        "#base id generation\n",
        "def process_products(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        # Generate baseid for each product\n",
        "        for product in data:\n",
        "            product_title = product['title']\n",
        "            vendor = product.get('vendor', 'unknown_vendor')\n",
        "            product_id = product.get('id', 'no_id')\n",
        "\n",
        "            # Generate initials and hash for baseid\n",
        "            title_initials = ''.join([word[0].upper() for word in product_title.split() if word])\n",
        "            vendor_initials = ''.join([word[0].upper() for word in vendor.split() if word])\n",
        "            id_hash = hashlib.md5(str(product_id).encode()).hexdigest()[:6]\n",
        "\n",
        "            baseid = f\"{title_initials}{vendor_initials}-{id_hash}\"\n",
        "            product['baseid'] = baseid\n",
        "\n",
        "            for variant in product.get('variants', []):\n",
        "                variant['baseid'] = baseid\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        # Add missing fields to original data if any exception occurs\n",
        "        for product in data:\n",
        "            if 'baseid' not in product:\n",
        "                product['baseid'] = None\n",
        "            for variant in product.get('variants', []):\n",
        "                if 'baseid' not in variant:\n",
        "                    variant['baseid'] = None\n",
        "        return data\n",
        "\n",
        "#clean options and title\n",
        "'''def replace_default_title(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        for product in products:\n",
        "            if \"options\" in product:\n",
        "                for option in product[\"options\"]:\n",
        "                    option[\"values\"] = [None if value and value.lower() == \"default title\" else value for value in option[\"values\"]]\n",
        "            if \"variants\" in product:\n",
        "                for variant in product[\"variants\"]:\n",
        "                    if variant[\"title\"] and variant[\"title\"].lower() == \"default title\":\n",
        "                        variant[\"title\"] = product[\"title\"]\n",
        "                    if variant[\"option1\"] and variant[\"option1\"].lower() == \"default title\":\n",
        "                        variant[\"option1\"] = None\n",
        "                    if variant[\"option2\"] and variant[\"option2\"].lower() == \"default title\":\n",
        "                        variant[\"option2\"] = None\n",
        "                    if variant[\"option3\"] and variant[\"option3\"].lower() == \"default title\":\n",
        "                        variant[\"option3\"] = None\n",
        "        return products\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while replacing default titles: {e}\")\n",
        "        return products  # Returning the original products list in case of an error'''\n",
        "\n",
        "def replace_default_title(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        for product in products:\n",
        "            if \"options\" in product:\n",
        "                for option in product[\"options\"]:\n",
        "                    option[\"values\"] = [None if value and value.lower() == \"default title\" else value for value in option[\"values\"]]\n",
        "            if \"variants\" in product:\n",
        "                for variant in product[\"variants\"]:\n",
        "                    try:\n",
        "                        if variant[\"title\"].strip() and variant[\"title\"].lower().strip() == \"default title\":\n",
        "                            variant[\"title\"] = product[\"title\"]\n",
        "                        if variant[\"option1\"].strip() and variant[\"option1\"].lower().strip() == \"default title\":\n",
        "                            variant[\"option1\"] = None\n",
        "                        if variant[\"option2\"].strip() and variant[\"option2\"].lower().strip() == \"default title\":\n",
        "                            variant[\"option2\"] = None\n",
        "                        if variant[\"option3\"].strip() and variant[\"option3\"].lower().strip() == \"default title\":\n",
        "                            variant[\"option3\"] = None\n",
        "                    except:\n",
        "                        #handling cases where optione1 is None in data\n",
        "                        pass\n",
        "        return products\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while replacing default titles: {e}\")\n",
        "        return products  # Returning the original products list in case of an error\n",
        "\n",
        "\n",
        "\n",
        "#inactive products detection\n",
        "def detect_inactive_products(data: List[Dict[str, Any]], months_threshold: int) -> List[Dict[str, Any]]:\n",
        "    threshold_date = datetime.datetime.now() - datetime.timedelta(days=months_threshold * 30)\n",
        "\n",
        "    for product in data:\n",
        "        updated_at = product.get('updated_at')\n",
        "        if updated_at:\n",
        "            try:\n",
        "                updated_at = datetime.datetime.strptime(updated_at, '%Y-%m-%dT%H:%M:%S%z')\n",
        "                updated_at = updated_at.replace(tzinfo=None)  # Remove timezone info\n",
        "                product['inactive_product'] = updated_at < threshold_date\n",
        "            except ValueError:\n",
        "                print(f\"Invalid date format for product: {product.get('id')}\")\n",
        "                product['inactive_product'] = False\n",
        "        else:\n",
        "            product['inactive_product'] = True\n",
        "\n",
        "    return data\n",
        "\n",
        "#discount extraction\n",
        "def extract_discounts_and_add_info(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        discounts = []\n",
        "        for product in data:\n",
        "            # Add discount code\n",
        "            product['discount_code'] = 'DISCOUNT2024'\n",
        "\n",
        "            # Extract discount information\n",
        "            product['discounts'] = []\n",
        "            for variant in product.get('variants', []):\n",
        "                price = float(variant['price'])\n",
        "                compare_at_price = variant.get('compare_at_price')\n",
        "                if compare_at_price and float(compare_at_price) != 0:\n",
        "                    compare_at_price = float(compare_at_price)\n",
        "                    discount = compare_at_price - price\n",
        "                    discount_percentage = (discount / compare_at_price) * 100\n",
        "                    variant['discount_percentage'] = discount_percentage  # Add discount_percentage to the variant\n",
        "                    product['discounts'].append({\n",
        "                        'price': price,\n",
        "                        'compare_at_price': compare_at_price,\n",
        "                        'discount': discount,\n",
        "                        'discount_percentage': discount_percentage\n",
        "                    })\n",
        "                    discounts.append({\n",
        "                        'product_id': product['id'],\n",
        "                        'title': product['title'],\n",
        "                        'price': price,\n",
        "                        'compare_at_price': compare_at_price,\n",
        "                        'discount': discount,\n",
        "                        'discount_percentage': discount_percentage\n",
        "                    })\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        # Add missing fields if an exception occurs\n",
        "        for product in data:\n",
        "            if 'discount_code' not in product:\n",
        "                product['discount_code'] = 'DISCOUNT2024'\n",
        "            if 'discounts' not in product:\n",
        "                product['discounts'] = []\n",
        "\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return data\n",
        "\n",
        "#price standardization\n",
        "def standardize_prices_to_usd(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        conversion_rates = get_conversion_rates()\n",
        "        if not conversion_rates:\n",
        "            logging.error(\"Conversion rates not available. Exiting.\")\n",
        "            return products\n",
        "\n",
        "        processed_products = process_products_for_usd(products, conversion_rates)\n",
        "        return processed_products\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in standardizing prices to USD: {e}\")\n",
        "        return products\n",
        "\n",
        "#freshness score\n",
        "def freshness_score(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    for product in data:\n",
        "        try:\n",
        "            created_date = parse_date(product.get('created_at'))\n",
        "            published_date = parse_date(product.get('published_at'))\n",
        "            updated_date = parse_date(product.get('updated_at'))\n",
        "\n",
        "            scores = []\n",
        "            now = datetime.datetime.now()\n",
        "\n",
        "            if created_date:\n",
        "                days_since_created = (now - created_date).days\n",
        "                scores.append(WEIGHTS['created'] * (MAX_DAYS - min(days_since_created, MAX_DAYS)) / MAX_DAYS)\n",
        "            if published_date:\n",
        "                days_since_published = (now - published_date).days\n",
        "                scores.append(WEIGHTS['published'] * (MAX_DAYS - min(days_since_published, MAX_DAYS)) / MAX_DAYS)\n",
        "            if updated_date:\n",
        "                days_since_updated = (now - updated_date).days\n",
        "                scores.append(WEIGHTS['updated'] * (MAX_DAYS - min(days_since_updated, MAX_DAYS)) / MAX_DAYS)\n",
        "\n",
        "            product['freshness_score'] = sum(scores) if scores else None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors du calcul du score de fraîcheur pour le produit {product.get('id', 'ID inconnu')}: {e}\")\n",
        "            product['freshness_score'] = None\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# def add_seller_info_to_json(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "#     # Placeholder function for extracting and scraping seller information\n",
        "#     return data\n",
        "\n",
        "def etl_ops(file_path: str) -> None:\n",
        "    try:\n",
        "        with open(file_path, 'r',encoding='utf-8', errors='ignore') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        print(\"Starting duplicate removal...\")\n",
        "        data = process_json_files(data)\n",
        "\n",
        "        print(\"Starting the keywords extraction...\")\n",
        "        data = keywords_extraction(data)\n",
        "\n",
        "        print(\"Detecting inactive products...\")\n",
        "        data = detect_inactive_products(data, months_threshold=3)\n",
        "\n",
        "        print(\"Starting the country and region detection\")\n",
        "        data = update_json_with_location(data)\n",
        "\n",
        "        print(\"Starting discount detection...\")\n",
        "        data = extract_discounts_and_add_info(data)\n",
        "\n",
        "        print(\"Starting price standardization...\")\n",
        "        data = standardize_prices_to_usd(data)\n",
        "\n",
        "        print(\"Adding min_price and max_price...\")\n",
        "        data = add_min_max_prices(data)\n",
        "\n",
        "        print(\"Replacing default titles...\")\n",
        "        data = replace_default_title(data)\n",
        "\n",
        "        print(\"starting freshness score calculation\")\n",
        "        data= freshness_score(data)\n",
        "\n",
        "        print(\"Starting base id generation...\")\n",
        "        data = process_products(data)\n",
        "\n",
        "\n",
        "\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from file: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {str(e)}\")\n",
        "\n",
        "# List of files to process\n",
        "file_paths = [\n",
        "    '/content/sample_data/products-celebratebytheyard-gift-2024-07-22-00-57.json',\n",
        "    '/content/sample_data/products-fowers-games-2024-07-22-00-59.json',\n",
        "    '/content/sample_data/products-obsessories-gift-2024-07-22-00-57.json',\n",
        "    '/content/sample_data/products-sally-gift-2024-07-22-00-55.json',\n",
        "    '/content/sample_data/products-talon-games-2024-07-22-00-59.json',\n",
        "    '/content/sample_data/products-theop-games-2024-07-22-01-00.json'\n",
        "\n",
        "]\n",
        "\n",
        "# Execute the processing\n",
        "for file_name in file_paths:\n",
        "    etl_ops(file_name)"
      ],
      "metadata": {
        "id": "KZFGZfcPqfSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84043e7f-37ae-4792-c259-8d693d4ed18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting duplicate removal...\n",
            "Starting the keywords extraction...\n",
            "Detecting inactive products...\n",
            "Starting the country and region detection\n",
            "Starting discount detection...\n",
            "Starting price standardization...\n",
            "Adding min_price and max_price...\n",
            "Replacing default titles...\n",
            "Starting base id generation...\n",
            "starting freshness score calculation\n",
            "Starting duplicate removal...\n",
            "Starting the keywords extraction...\n",
            "Detecting inactive products...\n",
            "Starting the country and region detection\n",
            "Starting discount detection...\n",
            "Starting price standardization...\n",
            "Adding min_price and max_price...\n",
            "Replacing default titles...\n",
            "Starting base id generation...\n",
            "starting freshness score calculation\n",
            "Starting duplicate removal...\n",
            "Starting the keywords extraction...\n",
            "Detecting inactive products...\n",
            "Starting the country and region detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-04 21:26:03,528 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - timed out\n",
            "2024-08-04 21:26:13,716 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - timed out\n",
            "2024-08-04 21:26:23,878 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - timed out\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting discount detection...\n",
            "Starting price standardization...\n",
            "Adding min_price and max_price...\n",
            "Replacing default titles...\n",
            "Starting base id generation...\n",
            "starting freshness score calculation\n",
            "Starting duplicate removal...\n",
            "Starting the keywords extraction...\n",
            "Detecting inactive products...\n",
            "Starting the country and region detection\n",
            "Starting discount detection...\n",
            "Starting price standardization...\n",
            "Adding min_price and max_price...\n",
            "Replacing default titles...\n",
            "Starting base id generation...\n",
            "starting freshness score calculation\n",
            "Starting duplicate removal...\n",
            "Starting the keywords extraction...\n",
            "Detecting inactive products...\n",
            "Starting the country and region detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-04 21:26:40,075 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:26:40,097 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:26:50,108 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - timed out\n",
            "2024-08-04 21:26:53,193 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:03,215 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - timed out\n",
            "2024-08-04 21:27:03,235 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:11,433 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:11,455 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:11,475 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:11,493 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:11,512 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:14,633 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:14,652 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:24,660 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - timed out\n",
            "2024-08-04 21:27:34,669 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - timed out\n",
            "2024-08-04 21:27:42,461 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:42,481 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:43,561 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:43,581 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:43,604 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:44,650 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:47,723 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:47,741 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:48,809 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:58,815 - whois.whois - ERROR - Error trying to connect to socket: closing socket - timed out\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - timed out\n",
            "2024-08-04 21:27:58,870 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:59,946 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:59,965 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:27:59,987 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:28:00,006 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:28:00,025 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:28:00,045 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:28:03,146 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:28:03,168 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "2024-08-04 21:28:03,187 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno 101] Network is unreachable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting discount detection...\n",
            "Starting price standardization...\n",
            "Adding min_price and max_price...\n",
            "Replacing default titles...\n",
            "Starting base id generation...\n",
            "starting freshness score calculation\n",
            "Starting duplicate removal...\n",
            "Starting the keywords extraction...\n",
            "Detecting inactive products...\n",
            "Starting the country and region detection\n",
            "Starting discount detection...\n",
            "Starting price standardization...\n",
            "Adding min_price and max_price...\n",
            "Replacing default titles...\n",
            "Starting base id generation...\n",
            "starting freshness score calculation\n"
          ]
        }
      ]
    }
  ]
}